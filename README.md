# TTS-related
Text-To-Sound related tools/paper/materials

Mp3 转 wav
ffmpeg -i 111.mp3 -acodec pcm_s16le -ac 1 -ar 16000 out.wav

Wav info
In linux:
  cmd:    file a.wav
./data_thchs30/data/5a1e8e9fd5df06236200ecf3.wav: Audio file with ID3 version 2.3.0MPEG ADTS, layer III,  v2.5,  24 kbps, 8 kHz, JntStereo

Kuyong’s. Css10.   10 Langs data set includes Chinese
https://github.com/Kyubyong/css10/blob/master/css10.v.1.0.pdf

Audio source:   LibriVox. https://librivox.org/
中文集 朝花夕拾的链接 https://librivox.org/chao-hua-si-she-by-lu-xun/

Audio editor audacity:
https://www.audacityteam.org

nohup python train.py >> out.txt 2>&1 &

Google tacotron(1,2) paper and demos
https://google.github.io/tacotron/

TTS（Text-To-Speech，语音合成）
链接：https://zhuanlan.zhihu.com/p/43944034
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

TTS（Text-To-Speech，语音合成），目前是一个“小而美”的AI领域，但我个人觉得非常有意思，感觉TTS在未来会被行业真正重视起来，并且会出现做得不错的创业公司。
本文，是我收集了很多线上/线下的相关信息后，提炼出的AI产品经理“最必要”了解的TTS技术知识和行业现状（多了没必要，少了又不足以入门、准备面试或工作实战）；不仅帮大家节省了时间，更是过滤了很多无用信息和过于技术的内容。
目录
一、核心概念
二、当前技术边界
三、瓶颈和机会（重点）
一、核心概念
1、TTS和ASR的概念区别
我们比较熟悉的ASR技术（Automatic Speech Recognition，语音识别），是将声音转化为文字，可类比于人类的耳朵。
而TTS技术（Text-To-Speech，语音合成），是将文字转化为声音（朗读出来），类比于人类的嘴巴。大家在Siri等各种语音助手中听到的声音，都是由TTS来生成的，并不是真人在说话。
TTS的技术实现方法，主要有2种：“拼接法”和“参数法”——
2、拼接法
1）定义：从事先录制的大量语音中，选择所需的基本单位拼接而成。这样的单位可以是音节、音素等等；为了追求合成语音的连贯性，也常常用使用双音子（从一个音素的中央到下一个音素的中央）作为单位。
2）优点：语音质量较高
3）缺点：数据库要求太大。一般需要几十个小时的成品预料。企业级商用的话，需要至少5万句，费用成本在几百万元。
3、参数法
1）定义：根据统计模型来产生每时每刻的语音参数（包括基频、共振峰频率等），然后把这些参数转化为波形。主要分为3个模块：前端、后端和声码器。
* 前端做的事情，是把文本进行解析，决定每个字的发音是什么，这句话用什么样的语气语调，用什么样的节奏来读，哪些地方是需要强调的重点等等。常见的语气相关的数据描述包含但不限于下面这些：韵律边界，重音，边界调，甚至情感。 还有更多的信息甚至是难以客观描述的，目前的算法只能暂且忽略。
* 注：拼接法和参数法，都有前端模块，拼接和参数的区别主要是后端声学建模方法的区别。
2）优点：数据库要求相对较小一些。
* 如果只需要出声（做demo），大概500句就可以，但是效果肯定不行。
* 通用TTS，一般至少需要5000句，6个小时（一般录制800句话，需要1个小时）。——从前期的准备、找人、找录音场地、录制、数据筛选、标注，最终成为“可以用的数据”，可能至少需要3个月。（讯飞在各方面比较成熟，用时会短很多）
* 个性化TTS，大多数是用“参数”方法的。（adobe、微软也有尝试过拼接法，不过相对参数方法来说不是太成熟，效果也并不是太通用）
3）缺点：质量比拼接法差一些。因为受制于发声算法，有损失。
* 因为主要弱点和难点就是声码器。声码器的作用是复现声音信号，难在重现声音细节，并且让人听不出各种杂音、沉闷、机械感等等。目前常见的声码器都是对声音信号本身作各种理论模型以及简化假设，可以说对细节的描述近似于忽略。
* 注：DeepMind的WaveNet，基本解决了声码器的问题。因为他们直接对语音样本进行预测，不依赖任何发音理论模型。最后出来的音质细节十分丰富，基本达到了与原始语音类似的音质水准（所谓质量提高了50%，就是这里），而且几乎可以对任意声音建模（这就太牛了）。
4、TTS的评判标准
1）主观测试（自然度），以MOS为主
A）MOS（Mean Opinion Scores），专家级评测（主观）；1-5分，5分最好。
* 注：微软小冰公开宣传是4.3分，但有业内朋友认为，也不能据此就说其“绝对”比科大讯飞好，因为每次评审的专家人选都不一样。说白了，目前整个AI行业内，还是各家说自己好的节奏。
B）ABX，普通用户评测（主观）。让用户来试听两个TTS系统，进行对比，看哪个好。
C）每次主观测评应该有区分。比如这次着重听多音字，下次主要听语气词等。
2）客观测试
A）对合成系统产生的声学参数进行评估，一般是计算欧式距离等（RMSE，LSD）。
B）对合成系统工程上的测试：实时率（合成耗时/语音时长）、首包响应时间（用户发出请求到用户感知到的第一包到达时间）、内存占用、CPU占用、3*24小时crash率等。
二、技术边界
1、通用TTS
1）在用户预期不苛刻的场景（APP/硬件），能满足商业化需求，比如语音助手/滴滴/高德/智能音箱/机器人）；但如果用户预期非常高的话，是很难满足的，因为还是会有“机器感/机械感”，不能非常自然的模拟人声。
2）目前行业各家公司的产品效果差不多，都基本能商用。
2、个性化TTS
1）在用户预期不苛刻的场景，能“基本”满足商业化需求，但是效果没通用TTS那么好。但如果用户预期非常高的话，暂时是满足不了的。
2）目前行业内能成熟商用的，主要还是科大讯飞，也有些创业公司在这个领域有所布局，如微量分贝（HEARD）这家致力于海量内容音频化的企业，对声音进行了分门别类的生成和储备，他们瞄准的企业级需求也会更为个性化、品牌化，诸如阿里巴巴旗下的“动物园”品牌（如天猫、闲鱼、盒马、菜鸟等），都会生成诸如“小猪佩奇”这样的角色化TTS 并被商用。
3、情感TTS
1）目前业界的情感合成更多了，是因为数据本身变多了、更有节奏了，超过了传统的播音风格，但并不是真正的“喜怒哀乐”等情感合成（想高兴就高兴的这种智能）。
2）在情感TTS的理论方面，学术界是有储备的，但是，整个行业目前都没怎么做（或者没做好），是因为情感TTS很依赖“情感意图识别”，“情感特征挖掘”、“情感数据”以及“情感声学技术”等，是个系统工程。其中第1点，即是和自然语言处理相关，比如需要知道“什么时侯该高兴或悲伤”；同时，具有情感演绎的语音数据的储备，也非常重要。
三、瓶颈和机会
主要有5个方向的瓶颈（同时也是机会）。
1、基础技术
1）TTS技术正处于重大变革：端到端（End-to-End）的TTS建模方法，加上WaveNet 的声码器思想，是未来TTS的发展方向。
* 端到端TTS，一般指tacotron，tacotron只是Google提出的合并了原先时长模型和声学模型的中段结构，可以接任何TTS前端和TTS后端。TTS前端如中文分词、注音、词性，都会提升tacotron性能；后端，参数、拼接、wavenet都可以选用。
* 关于WaveNet技术的商业化：Google今年初将第二代WaveNet技术商业化了，速度比第一代快一万倍。而国内各家公司，基本也仿制出来了（论文算法），但工程化还需要时间，而且成本还是太高，短期内应该没法商用。
* 关于效果：TTS最终效果好坏，技术只占50%不到，在技术都差不多的情况下，声优质量和数据量最重要，其次是相同部署规模和成本的TTS才能相互比较，即，不能简单的说哪家公司的效果比另一家更好，a）比如，拿百度/腾讯/阿里/图灵等很多家AI公司的WaveNet v1的效果，一般都能超过讯飞线上的接口，但部署成本高几万倍，且不实时；WaveNet V2商业化以后，虽然能实时，但部署成本至少也比高配拼接TTS高10倍左右。b）成本，部分和采样率相关，例如，讯飞/百度TTS的采样率都是16k，如果用24k和48k，主观体验至少强50%，但成本会翻倍；也就是说，其他AI公司的24kTTS的MOS，能吊打讯飞/百度的API，但不能说他们的技术就比讯飞/百度强，因为在商业化时，会牺牲效果来降低成本。
2）如何让离线版效果达到在线版水平。很多客户希望（奢望）有离线版本，并且效果和在线版本一样好……现阶段来说，可能真是“臣妾做不到啊”。
2、数据缺乏
一方面，特别是个性化TTS，需要数据量更大。比如默认男孩声音，要转成女孩，就比较难。
另一方面，数据的获取（制作）成本和周期，也是各家在初期的竞争着力点，比如，一般来说，一款（套）TTS数据，至少需要先录制2-3万句话，再加上数据标注，通常耗时在3个月以上（且需要主播全力配合），对于30小时的数据，价格通常在30-50万，而上文提到的微量分贝（HEARD）这家公司，调动了8000+位优质播音人员，在给不同内容配音的同时，也做了大量结构化数据的存储（库存化），这样，针对大部分客户的数据需求，并不需要再找主播进行录制，而是直接从仓库调取数据进行解冻即可（数据标注）；通过将这种 “边进行业务边赚取数据”的流程标准化，其获取数据的成本大大降低到行业的五分之一 ，并且一旦有需求，可以在1个月内进行交付。这家公司在南方搭建的数据标注工场的规模，也是巨大的，包括华为等公司都从其采购语音合成数据。
3、人才匮乏
不仅没法跟NLP、CV等热门AI人才比，就算跟同样不算热门的ASR比，TTS的人才都还要少一些。
4、产品化难度
由于技术限制，现阶段不可能有非常完美的TTS效果，所以
1）尽量选择用户预期不苛刻的场景，或者在产品体验设计时，管理好用户预期（比如打车软件，郭德纲/林志玲的声音，差不多就行）。
2）选择“参数法”还是“拼接法”，和公司的技术储备、成本、以及产品目标相关。在垂直领域，现有的TTS技术（参数或者拼接）都可以针对产品做得很好。现在行业还没有太好的效果，很大原因是因为产品经理还没有深入介入，有很多细节的坑要踩（产品设计+工程化实现）——未来应该会有惊艳的产品出现。
3）体验细节设计，和一般互联网产品很不同，比如
A）文案设计，非常重要；因为在语音交互场景，不能太长，用户没耐心和时间听完的。
B）可以加入背景音乐，掩盖杂音等细节瑕疵。
C）特殊场景，还有特别的需求，比如远场场景和戴耳机场景相比，还是会有区别的。
D）中英文混合TTS。比如用户想播首英语歌曲，困难在于：所有中文的发音当中，中文和英文合拍念出来是很难的，为什么呢？因为往往录音的人。录中文是一批人，录英文又是一批人。两种语言结合起来，再用机器学习学出来，声音就会变得非常怪。这方面，小雅音箱曾经花了很大的精力和成本去“死磕”解决，详见《傅盛：人工智能的破局点是技术和产品结合【猎户星空发布小雅语音 OS】》。
5、商业化压力
1）如果要有足够的市场竞争力，至少需要12个月的时间，2~6人团队（如果有人做过前端相关工作，会节省巨大成本——工作量主要在中文前端NLP部分，比如分词、注音、词性文本规整化等），几百万资金投入（1个GPU一年十万，支持并发只有几十个）。并且，大公司的先发优势巨大，小公司必须切细分场景。
2）我个人认为，个性化TTS、情感TTS会在各细分场景得到更大的应用，比如知识付费、明星IP、智能硬件、车联网、实体/虚拟机器人等。


Tacotron 评价

模型复杂。用一个模型端到端方式合成语音，虽然省去了中间步骤，但模型复杂，不好调试，不好训练
模型除错难。训练再好的模型，也可能对某些文本发音错误，这时，想要纠正这些错误，很难；需要重新准备数据、再次训练，再次训练也不一定能克服那些问题，代价很大。
人为干预能力差。参数合成往往可以人为指定语速、重音、断句、停顿、韵律等信息，进行个性化合成。然而，端到端合成这些信息全部由模型自己学习，很难加入人为控制。因此，很难产品化。
端到端不彻底。严格意义上讲，Tacotron也并非端到端，模型输出的是梅尔频谱(mel-scale spectrogram)，再用CBHG结构转为频谱幅度(spectral magnitude)，最后使用了Griffin-Lim这样的vocoder转为最终的音频。对此，好处是Tacotron中的seq2seq结构部分更容易训练，后处理部分可以单独训练，缺点是，后处理网络和Griffin-Lim本身的局限会影响到音质。


端到端的语音合成存在着以下问题。
1.韵律结构不稳定。一般使用端到端的网络，可能使用的是十几个小时的数据集，大概5000到8000条数据左右，这样的数据量跟专门做韵律分析的模型样本比较，实在太少了。所以想直接从这几千条想得到一个比较稳定的韵律预测，其实是一个很困难的事情。在实验中也可以发现，从合成的自然度来看，端到端的语音合成在部分样本上可以真人达到无法分辨的效果，有一些样本合成出来韵律比较随机，根本放到业务上正常使用。例如明天气温19度。用端到端的网络，合成出来的可能会变成，明_天气_温19度，只要训练集里天气是一个比较高频的词。如果加上各种附加条件，例如韵律信息，时长信息，那这个也不算是严格意义上的端到端网络了。
2.性能方面。真正要放到业务上使用，非常看重可以同时跑多少路的问题。目前，用传统的，拼接或者参数法，合成的一部20万字的长篇小说，成本大约是五块钱以下。如果大规模的使用神经网络，在这方面，成本会大大增加。
3.稳定性隐患。端到端的网络，有一个很明显的缺点在于不好调试。太依赖神经网络的自我学习能力，一旦出了错误，不好定位。如果真的要投到业务中使用没有，需要经过几万几十万的，大规模测试，发错音错字，跳字，或者停顿混乱这样的风险系数非常大。以tacotron为例，从Bahdanau arrention到location attention再到现在的forward attention，错漏字的情况明显好转了，但是仍然有缺陷。
对于跨域种和多特征人少样本这种应用场景下，端到端的网络仍然有明显优势。


